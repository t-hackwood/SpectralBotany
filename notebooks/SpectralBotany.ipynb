{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral Botany proof of concept\n",
    "\n",
    "By Tim Hackwood\n",
    "\n",
    "This Notebook demonstrates techniques for creating spectral species over the Brigalow Belt bioregion in Queensland, Australia and how they may be used to model the distribution of plant species with multispectral remote sensing data.\n",
    "\n",
    "Methodology for raster segmentation and clustering adapted from Peter Scarth (2022) https://github.com/petescarth/segmentation-example, with pyshepseg https://github.com/ubarsc/pyshepseg leveraged to segment the large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from rios import applier, cuiprogress\n",
    "import pickle\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "from shapely import box\n",
    "import rasterio\n",
    "from rasterio.plot import reshape_as_image\n",
    "import numpy as np\n",
    "import os\n",
    "import galah\n",
    "from osgeo import gdal, ogr, gdalconst, osr\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "from rios import rat\n",
    "from rios import applier, cuiprogress\n",
    "from rios import ratapplier\n",
    "from pyshepseg import tiling\n",
    "from pyshepseg import tilingstats\n",
    "from pyshepseg import utils\n",
    "from pystac_client import Client\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "# from skimage import exposure\n",
    "import shapely.geometry\n",
    "import branca\n",
    "\n",
    "# Set the working directory to SpectralBotany\n",
    "os.chdir(\"/home/tim/rubella/scripts/SpectralBotany\")\n",
    "\n",
    "# Config for Atlas Living Australia APO\n",
    "galah.galah_config(email = \"timothy.hackwood@gmail.com\", atlas = \"Australia\")\n",
    "\n",
    "AOI = \"data/SpectralBotanyTest.gpkg\"\n",
    "Layer = \"BrigalowBeltAOI\"\n",
    "\n",
    "# FIles for PCA and Segment outputs\n",
    "RASTER_PC = \"/home/tim/dentata/SAsegs/GeoscapeAOIPCA.tif\" # 250m version for demonstration purposes\n",
    "RASTER_SEG = RASTER_PC.replace(\".tif\", \"_segs.kea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AOI\n",
    "brigalow = gpd.read_file(AOI, layer=Layer)\n",
    "# Reproject to WGS84 for STAC API\n",
    "brigalow.to_crs(4326, inplace=True)\n",
    "# Plot to check everything looks ok\n",
    "brigalow.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rgb_to_html(input_tif, output_html, zoom=8, nodata=0, bandorder=[1, 2, 3], show=True):\n",
    "    \"\"\"\n",
    "    Warp tif to wgs84, normalize data and stretch autimatically, and create a folium map with the raster overlay \n",
    "    \"\"\"\n",
    "    # Virtually warp to wgs84\n",
    "    PCA_web = input_tif.replace(\".tif\", \"_4326.vrt\")\n",
    "    # Define the target CRS as EPSG:4326 (WGS 84)\n",
    "    target_crs = 'EPSG:4326'\n",
    "    # Reproject the raster in memory\n",
    "    gdal.Warp(PCA_web, input_tif, dstSRS=target_crs, format='VRT')\n",
    "\n",
    "    # Open the reprojected raster file with rasterio\n",
    "    with rasterio.open(PCA_web) as src:\n",
    "        # Read the first three bands\n",
    "        data = src.read(bandorder).astype(np.float32)\n",
    "        # Identify the no-data values\n",
    "        no_data_mask = np.all(data == nodata, axis=0)\n",
    "        # Normalize data to [0, 1] for each band before applying CLAHE\n",
    "        data_normalized = np.array([exposure.rescale_intensity(band, in_range=(band.min(), band.max()), out_range=(0, 1)) for band in data])\n",
    "        # Apply adaptive histogram equalization (CLAHE) for better contrast\n",
    "        data_equalized = np.array([exposure.equalize_adapthist(band, clip_limit=0.03) for band in data_normalized])\n",
    "        # Reshape data for plotting\n",
    "        data = reshape_as_image(data_equalized)\n",
    "        # Add alpha channel: 0 where no-data, 1 elsewhere\n",
    "        alpha_channel = np.where(no_data_mask, 0, 1).astype(np.uint8)\n",
    "        data = np.dstack((data, alpha_channel))\n",
    "        # Get bounds of the raster\n",
    "        bounds = src.bounds\n",
    "        top_left = [bounds.top, bounds.left]\n",
    "        bottom_right = [bounds.bottom, bounds.right]\n",
    "\n",
    "    # Create a base map\n",
    "    m = folium.Map(location=[(top_left[0] + bottom_right[0]) / 2,\n",
    "                             (top_left[1] + bottom_right[1]) / 2],\n",
    "                   zoom_start=zoom)\n",
    "\n",
    "    # Add the raster image as an overlay\n",
    "    raster_layer = folium.raster_layers.ImageOverlay(\n",
    "        image=data,\n",
    "        bounds=[[bottom_right[0], top_left[1]], [top_left[0], bottom_right[1]]],\n",
    "        opacity=1,\n",
    "        interactive=True,\n",
    "        cross_origin=False,\n",
    "        zindex=1\n",
    "    )\n",
    "    raster_layer.add_to(m)\n",
    "\n",
    "    # Add Layer Control\n",
    "    folium.LayerControl().add_to(m)\n",
    "\n",
    "    # Save map to HTML file\n",
    "    m.save(output_html)\n",
    "    \n",
    "    if show:\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_thematic_to_html(input_tif, output_html, zoom=8, nodata=0, cmap='viridis', legend_range=None, show=False):\n",
    "    \"\"\"\n",
    "    Warp tif to wgs84, normalize data and stretch automatically, and create a folium map with the raster overlay.\n",
    "    \"\"\"\n",
    "    # Virtually warp to wgs84\n",
    "    PCA_web = input_tif.replace(\".tif\", \"_4326.vrt\")\n",
    "    # Define the target CRS as EPSG:4326 (WGS 84)\n",
    "    target_crs = 'EPSG:4326'\n",
    "    # Reproject the raster in memory\n",
    "    gdal.Warp(PCA_web, input_tif, dstSRS=target_crs, format='VRT')\n",
    "\n",
    "    # Open the reprojected raster file with rasterio\n",
    "    with rasterio.open(PCA_web) as src:\n",
    "        # Read the first band\n",
    "        data = src.read(1).astype(np.float32)\n",
    "        # Identify the no-data values\n",
    "        no_data_mask = (data == nodata)\n",
    "        # Apply quantile stretch only to non no-data values\n",
    "        valid_data = data[~no_data_mask]\n",
    "        p2, p98 = np.percentile(valid_data, (2, 98))\n",
    "        data_stretched = exposure.rescale_intensity(data, in_range=(p2, p98), out_range=(0, 1))\n",
    "        # Apply adaptive histogram equalization (CLAHE) for better contrast\n",
    "        data_equalized = exposure.equalize_adapthist(data_stretched, clip_limit=0.03) \n",
    "        # Apply colormap\n",
    "        cmap = plt.get_cmap(cmap)\n",
    "        data_colored = cmap(data_equalized)\n",
    "        # Add alpha channel: 0 where no-data, 1 elsewhere\n",
    "        alpha_channel = np.where(no_data_mask, 0, 1).astype(np.uint8)\n",
    "        data_colored[..., -1] = alpha_channel\n",
    "        # Get bounds of the raster\n",
    "        bounds = src.bounds\n",
    "        top_left = [bounds.top, bounds.left]\n",
    "        bottom_right = [bounds.bottom, bounds.right]\n",
    "\n",
    "    # Create a base map\n",
    "    m = folium.Map(location=[(top_left[0] + bottom_right[0]) / 2,\n",
    "                             (top_left[1] + bottom_right[1]) / 2],\n",
    "                   zoom_start=zoom,\n",
    "                   )\n",
    "\n",
    "    # Add the raster image as an overlay\n",
    "    raster_layer = folium.raster_layers.ImageOverlay(\n",
    "        image=data_colored,\n",
    "        bounds=[[bottom_right[0], top_left[1]], [top_left[0], bottom_right[1]]],\n",
    "        opacity=1,\n",
    "        interactive=True,\n",
    "        cross_origin=False,\n",
    "        zindex=1\n",
    "    )\n",
    "    raster_layer.add_to(m)\n",
    "\n",
    "    # Add Layer Control\n",
    "    folium.LayerControl().add_to(m)\n",
    "\n",
    "    # Determine legend range\n",
    "    legend_min = legend_range[0] if legend_range else p2\n",
    "    legend_max = legend_range[1] if legend_range else p98\n",
    "\n",
    "    # Add a legend\n",
    "    colormap = branca.colormap.LinearColormap(\n",
    "        colors=[cmap(i) for i in range(cmap.N)],\n",
    "        vmin=legend_min,\n",
    "        vmax=legend_max,\n",
    "        caption='Legend'\n",
    "    )\n",
    "    colormap.add_to(m)\n",
    "\n",
    "    # Save map to HTML file\n",
    "    m.save(output_html)\n",
    "    \n",
    "    if show:\n",
    "        return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Landsat data if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build vrt by band from DEA files on STAC API for each band\n",
    "def buildVRT(tiles, dir):\n",
    "\n",
    "    # List of the band names to stack in the VRT\n",
    "    BandNames = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']\n",
    "    \n",
    "    vrtList = []\n",
    "\n",
    "    # Make a full timeseries vrt with each band\n",
    "    for band in BandNames:\n",
    "        vrtName = band + '_.vrt'\n",
    "        vrt = os.path.join(dir, vrtName)\n",
    "        vrtList.append(vrt)\n",
    "        urlList = []\n",
    "        for item in tiles:\n",
    "            # Get the url for the band\n",
    "            url = item.assets[band].href\n",
    "            # Add it to the list\n",
    "            urlList.append(url.replace(\n",
    "                's3://',\n",
    "                '/vsis3/'))\n",
    "        # Set options for the VRT file\n",
    "        vrtOptions = gdal.BuildVRTOptions(resampleAlg='nearest',\n",
    "                                        separate=False,\n",
    "                                        resolution='highest',\n",
    "                                        VRTNodata=-999)\n",
    "\n",
    "        # Then make the VRT file using the list of URLs\n",
    "        vrtData = gdal.BuildVRT(vrt,\n",
    "                                urlList,\n",
    "                                options=vrtOptions)\n",
    "        \n",
    "        # Close the VRT file\n",
    "        vrtData = None\n",
    "\n",
    "    return vrtList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get landsat-8 data (using DEA barest-earth)\n",
    "brigalow.to_crs(4326, inplace=True)\n",
    "bbox = brigalow.total_bounds\n",
    "\n",
    "# STAC querey\n",
    "client = Client.open(\"https://explorer.sandbox.dea.ga.gov.au/stac\")\n",
    "s2Search = client.search(\n",
    "    bbox=bbox,\n",
    "    collections=['ls8_barest_earth_albers'],\n",
    ")\n",
    "# Show the results of the search\n",
    "print(f\"{s2Search.matched()} items found\")\n",
    "\n",
    "tiles = s2Search.item_collection()\n",
    "\n",
    "# Build the virtual mosaic for each band\n",
    "print(\"Building virtual band mosaics\")\n",
    "vrtList = buildVRT(tiles, 'data/Landsat')\n",
    "\n",
    "vrtOptions = gdal.BuildVRTOptions(resampleAlg='nearest',\n",
    "                                separate=True,\n",
    "                                resolution='highest',\n",
    "                                VRTNodata=-999)\n",
    "print(\"Stacking bands\")\n",
    "vrtData = gdal.BuildVRT('data/Landsat/LandsatBarest_brigalow.vrt',\n",
    "                        vrtList,\n",
    "                        options=vrtOptions)\n",
    "\n",
    "# Close the VRT file\n",
    "vrtData = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a 250m resampled image for this demonstration\n",
    "Landsat = \"data/Landsat/LandsatBarest_brigalow.vrt\".replace(\".vrt\", \"_250m.tif\")\n",
    "gdal.Warp(Landsat, \"data/Landsat/LandsatBarest_brigalow.vrt\", xRes=250, yRes=250, resampleAlg=\"nearest\", format='GTiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export landsat to a web map\n",
    "plot_rgb_to_html(Landsat, \"LandsatBarest_brigalow.html\", zoom=8, nodata=-999, bandorder=[3, 2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack with indices and get PCA for segmentation\n",
    "\n",
    "The 6 Landsat bands are stacked with NDVI and NDWI indices. For future versions, other indicies and timeseries data would be good to add in.\n",
    "\n",
    "A PCA tiff is produced based on this data stack, then segmented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New version using seasonal Sentinel 2\n",
    "import glob\n",
    "\n",
    "# Calculate the bounding box of brigalow (Landsat imagery is in Albers)\n",
    "brigalow.to_crs(epsg=3577, inplace=True)\n",
    "bbox = brigalow.total_bounds\n",
    "\n",
    "s2 = glob.glob(\"/home/tim/dentata/Sentinel2_seasonal/*2023*.vrt\")\n",
    "\n",
    "datastack = []\n",
    "\n",
    "for raster in s2:\n",
    "    with rasterio.open(raster) as src:\n",
    "        \n",
    "        # Open a subsampled version of the image\n",
    "        out_height = int(src.height / 100)\n",
    "        out_width = int(src.width / 100)\n",
    "\n",
    "        # Read the subsampled data from src using the bounding box\n",
    "        img = src.read(window=rasterio.windows.from_bounds(*bbox, transform=src.transform), out_shape=(src.count, out_height, out_width))\n",
    "        inshape = img.shape\n",
    "        print(inshape)\n",
    "        ndvi = (img[3]-img[2])/(img[3]+img[2]+np.finfo(float).eps).reshape(1, inshape[1], inshape[2])\n",
    "        ndwi = (img[1]-img[5])/(img[1]+img[5]+np.finfo(float).eps).reshape(1, inshape[1], inshape[2])\n",
    "            \n",
    "        # Stack these ratios with the original data\n",
    "        stack = np.vstack([img, ndvi, ndwi])\n",
    "        \n",
    "        datastack.append(stack)\n",
    "        \n",
    "datastack = np.array(datastack)\n",
    "\n",
    "print(datastack.shape)\n",
    "\n",
    "# Calculate the mean and std across all images (axis 0) for each band\n",
    "mean_bands = np.mean(datastack, axis=0)\n",
    "std_bands = np.std(datastack, axis=0)\n",
    "\n",
    "# Stack the mean and std arrays to form a new data stack\n",
    "new_datastack = np.concatenate([mean_bands, std_bands], axis=0)\n",
    "\n",
    "\n",
    "print(new_datastack.shape)\n",
    "\n",
    "# get mean and std for each band datastack\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a subsampled dataset of multispectral imagery for PCA\n",
    "with rasterio.open(\"/home/tim/dentata/Sentinel2_seasonal/cvmsre_qld_m202312202402_abma2.vrt\") as src:\n",
    "    # Calculate the bounding box of brigalow (Landsat imagery is in Albers)\n",
    "    brigalow.to_crs(epsg=3577, inplace=True)\n",
    "    bbox = brigalow.total_bounds\n",
    "    # Open a subsampled version of the image\n",
    "    out_height = int(src.height / 50)\n",
    "    out_width = int(src.width / 50)\n",
    "\n",
    "    # Read the subsampled data from src using the bounding box\n",
    "    img = src.read(window=rasterio.windows.from_bounds(*bbox, transform=src.transform), out_shape=(src.count, out_height, out_width))\n",
    "    inshape = img.shape\n",
    "    print(inshape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indicies and fit the PCA\n",
    "\n",
    "N_COMPONENTS = 3 # Change depending on variance.\n",
    "\n",
    "# Calculate NDVI/NDWI, stack and scale\n",
    "# Landsat-8 bands - TERN Sentinel 2 seasonal surface reflectance is the same as landsat 8\n",
    "ndvi = (img[3]-img[2])/(img[3]+img[2]+np.finfo(float).eps).reshape(1, inshape[1], inshape[2])\n",
    "ndwi = (img[1]-img[5])/(img[1]+img[5]+np.finfo(float).eps).reshape(1, inshape[1], inshape[2])\n",
    "\n",
    "    \n",
    "# Stack these ratios with the original data\n",
    "stack = np.vstack([img, ndvi, ndwi])\n",
    "print(stack.shape)\n",
    "\n",
    "# Reshape stack for scaling\n",
    "scale = np.reshape(stack, (stack.shape[0], -1))\n",
    "\n",
    "\n",
    "\n",
    "# Delete unneeded variables\n",
    "del img\n",
    "del ndvi\n",
    "del ndwi\n",
    "\n",
    "# Rescale with robust scaler    \n",
    "scaler = RobustScaler()\n",
    "stack_scale = scaler.fit_transform(scale.T)\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, './stack_scaler.pkl')\n",
    "print(stack_scale.shape)\n",
    "\n",
    "del scale\n",
    "\n",
    "# Fit the PCA\n",
    "pca = PCA(n_components=N_COMPONENTS, svd_solver='full', whiten=True)\n",
    "pca.fit(stack_scale)\n",
    "# Print the variance and save the PCA\n",
    "sumvar = np.cumsum(pca.explained_variance_ratio_)\n",
    "print(f\"Variance explained: {sumvar}\")\n",
    "joblib.dump(pca, './pca.pkl')\n",
    "\n",
    "# Transform the data to byte scale and save\n",
    "pcaData = pca.transform(stack_scale).T\n",
    "byteScale = np.percentile(pcaData,[0.001,99.999],axis=1)\n",
    "joblib.dump(byteScale, './byteScale.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the PCA to the full image and mask to Brigalow AOI\n",
    "# Takes ~20 minutes\n",
    "def _applyPCA(info, inputs, outputs, otherargs):\n",
    "    \"\"\"\n",
    "    Apply PCA to full resolution dataset.\n",
    "    \"\"\"\n",
    "    # Open the images\n",
    "    s2 = inputs.inlist\n",
    "    img = s2[0]\n",
    "    inshape = img.shape\n",
    "    aoi = inputs.aoi\n",
    "    nodata = np.any(img == 0, axis=0)\n",
    "\n",
    "    datastack = []\n",
    "\n",
    "    for img in s2:\n",
    "\n",
    "        inshape = img.shape\n",
    "        ndvi = (img[3]-img[2])/(img[3]+img[2]+np.finfo(float).eps).reshape(1, inshape[1], inshape[2])\n",
    "        ndwi = (img[1]-img[5])/(img[1]+img[5]+np.finfo(float).eps).reshape(1, inshape[1], inshape[2])\n",
    "            \n",
    "        # Stack these ratios with the original data\n",
    "        stack = np.vstack([img, ndvi, ndwi])\n",
    "        \n",
    "        datastack.append(stack)\n",
    "            \n",
    "    datastack = np.array(datastack)\n",
    "\n",
    "    # Calculate the mean and std across all images (axis 0) for each band\n",
    "    mean_bands = np.mean(datastack, axis=0)\n",
    "    std_bands = np.std(datastack, axis=0)\n",
    "    stdOut = 10000 * 10000 + std_bands\n",
    "    print(stdOut.shape)\n",
    "    stdOut[:, np.any(nodata)] = 0\n",
    "    outputs.ndviSD = stdOut.astype(np.uint16)\n",
    "\n",
    "    # Stack the mean and std arrays to form a new data stack\n",
    "    stack = np.concatenate([mean_bands, std_bands], axis=0)\n",
    "\n",
    "    scaled_stack = np.reshape(stack, (stack.shape[0], -1)).astype('float32').T\n",
    "\n",
    "    # Apply the PCA    \n",
    "    pc = otherargs.pca.transform(otherargs.scaler.transform(scaled_stack))\n",
    "    # Rescale to 16bit\n",
    "    pc = np.round(np.clip(1.0 + 65534.0\n",
    "                        * (pc-otherargs.bytescale[0])\n",
    "                        / (otherargs.bytescale[1]-otherargs.bytescale[0])\n",
    "                        ,1,65535))\n",
    "        \n",
    "    # Reshape the output\n",
    "    pc = np.reshape(pc.T,(pc.shape[1],inshape[1],inshape[2]))\n",
    "    # Mask the output for no data\n",
    "\n",
    "    pc[:,np.any(stack == otherargs.noData,axis=0)] = 0\n",
    "    pc[:, np.any(aoi == 0, axis=0)] = 0 # mask to the AOI\n",
    "    outputs.pc =  pc.astype(np.uint16)\n",
    "\n",
    "# Get the no data value\n",
    "ds = gdal.Open(\"/home/tim/dentata/Sentinel2_seasonal/cvmsre_qld_m202109202111_abma2.vrt\")\n",
    "noData = ds.GetRasterBand(1).GetNoDataValue()\n",
    "    \n",
    "# Create the RIOS file objects\n",
    "infiles = applier.FilenameAssociations()\n",
    "outfiles = applier.FilenameAssociations()\n",
    "\n",
    "# Setup the IO\n",
    "infiles.inlist = glob.glob(\"/home/tim/dentata/Sentinel2_seasonal/*2023*.vrt\")\n",
    "infiles.aoi = \"data/SpectralBotanyTest.gpkg\"\n",
    "\n",
    "outfiles.pc = RASTER_PC\n",
    "outfiles.ndviSD = \"./NDVI_SD\"\n",
    "\n",
    "# Get the otherargs\n",
    "otherargs = applier.OtherInputs()\n",
    "otherargs.pca = joblib.load(\"pca.pkl\")\n",
    "otherargs.scaler = joblib.load(\"stack_scaler.pkl\")\n",
    "otherargs.bytescale = joblib.load(\"byteScale.pkl\")\n",
    "otherargs.noData = noData\n",
    "\n",
    "# Controls for the processing   \n",
    "controls = applier.ApplierControls()\n",
    "controls.vectorlayer = \"BrigalowBeltAOI\"\n",
    "controls.setBurnValue = 1\n",
    "controls.windowxsize = 512\n",
    "controls.windowysize = 512\n",
    "controls.setStatsIgnore(0) #  nodata\n",
    "controls.progress = cuiprogress.CUIProgressBar()\n",
    "controls.setFootprintType(\"INTERSECTION\")\n",
    "controls.setResampleMethod(\"near\")\n",
    "controls.setOutputDriverName(\"GTIFF\")\n",
    "controls.setCreationOptions([\"COMPRESS=DEFLATE\",\n",
    "                                \"ZLEVEL=9\",\n",
    "                                \"PREDICTOR=2\",\n",
    "                                \"BIGTIFF=YES\",\n",
    "                                \"TILED=YES\",\n",
    "                                \"INTERLEAVE=BAND\",\n",
    "                                \"NUM_THREADS=ALL_CPUS\",\n",
    "                                \"BLOCKXSIZE=512\",\n",
    "                                \"BLOCKYSIZE=512\"])\n",
    "\n",
    "# Set concurrency depending on system\n",
    "conc = applier.ConcurrencyStyle(numReadWorkers=3,\n",
    "                                numComputeWorkers=2,\n",
    "                                computeWorkerKind=\"CW_THREADS\",\n",
    "                                readBufferPopTimeout=120,\n",
    "                                computeBufferPopTimeout=120\n",
    "                                )\n",
    "\n",
    "controls.setConcurrencyStyle(conc)\n",
    "\n",
    "# Run the function\n",
    "print(\"Processing PCA\")\n",
    "applier.apply(_applyPCA, infiles, outfiles, otherargs, controls=controls)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a 250m resampled image for this demonstration\n",
    "PCA_250m = RASTER_PC.replace(\".tif\", \"_250m.tif\")\n",
    "gdal.Warp(PCA_250m, RASTER_PC, xRes=250, yRes=250, resampleAlg=\"nearest\", format='GTiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result to a slippy map\n",
    "plot_rgb_to_html(PCA_250m, \"PCA_250m.html\", zoom=8, show=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment the PCA to get Spectral Species \n",
    "\n",
    "This segmentation method uses a tiled version of Shepard segmentation and is great for larger dataset as the data are chunked into managable sizes before being stitched back together.\n",
    "\n",
    "Shepard segmentation is based on k-means clustering and seems functionally identical to methods used for spectral species extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get NDVI and NDMI distributions to add into spatial statistics\n",
    "def indexDist(vrtList):\n",
    "\n",
    "    ndvistack = []\n",
    "    ndwistack = []\n",
    "\n",
    "    for img in vrtList:\n",
    "\n",
    "        inshape = img.shape\n",
    "        ndvi = (img[3]-img[2])/(img[3]+img[2]+np.finfo(float).eps).reshape(1, inshape[1], inshape[2])\n",
    "        ndwi = (img[1]-img[5])/(img[1]+img[5]+np.finfo(float).eps).reshape(1, inshape[1], inshape[2])\n",
    "            \n",
    "        # Stack these ratios with the original data\n",
    "        ndvistack.append(ndvi)\n",
    "        ndwistack.append(ndwi)\n",
    "            \n",
    "    ndvistack = np.stack(ndvistack, axis=0)\n",
    "    ndwistack = np.stack(ndwistack, axis=0)\n",
    "\n",
    "    # Get new stack with 5th, 25th, 50th, 75th and 95th percentiles\n",
    "    ndviStats = np.percentile(ndvistack, [5, 25, 50, 75, 95], axis=0)\n",
    "    ndwiStats = np.percentile(ndwistack, [5, 25, 50, 75, 95], axis=0)\n",
    "\n",
    "    return ndviStats, ndwiStats\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEGMENT_SIZE = 100 # Approximately 1 hectare minimum segment size @ 10m pixels \n",
    "N_COMPONENTS = 4 # Number of PCA components\n",
    "\n",
    "\n",
    "# Segment using the tiled version\n",
    "print('Segmenting')\n",
    "tiledSegResult = tiling.doTiledShepherdSegmentation(RASTER_PC, RASTER_SEG,\n",
    "            minSegmentSize=SEGMENT_SIZE, numClusters=512,\n",
    "            bandNumbers=None, subsamplePcnt=None,\n",
    "            maxSpectralDiff='auto', spectDistPcntile=25,\n",
    "            imgNullVal=0, fourConnected=True, verbose=True,\n",
    "            simpleTileRecode=False, outputDriver='KEA', kmeansObj=None)\n",
    "           \n",
    "# Do histogram, stats and colour table on final output file.\n",
    "print('Adding Histogram, Stats and Colour Table')\n",
    "outDs = gdal.Open(RASTER_SEG, gdal.GA_Update)\n",
    "hist = tiling.calcHistogramTiled(outDs, tiledSegResult.maxSegId, writeToRat=True)\n",
    "band = outDs.GetRasterBand(1)\n",
    "utils.estimateStatsFromHisto(band, hist)\n",
    "utils.writeRandomColourTable(band, tiledSegResult.maxSegId+1)\n",
    "del outDs\n",
    "\n",
    "# Add the original Image Stats\n",
    "print('Adding Image Stats')\n",
    "for imgbandnum in range(1,N_COMPONENTS+1):\n",
    "    statsSelection = [\n",
    "        ('pca_Band_{}_mean'.format(imgbandnum), 'mean'),\n",
    "        ('pca_Band_{}_std'.format(imgbandnum), 'stddev')]\n",
    "    tilingstats.calcPerSegmentStatsTiled(RASTER_PC, imgbandnum, RASTER_SEG,statsSelection)\n",
    "\n",
    "# Export the segmentation as tifs for visualisation\n",
    "print('Exporting Segmentation')\n",
    "infiles = applier.FilenameAssociations()\n",
    "outfiles = applier.FilenameAssociations()   \n",
    "infiles.image = RASTER_SEG\n",
    "outfiles.id = RASTER_SEG.replace('.kea', '_id.tif')\n",
    "outfiles.rgb = RASTER_SEG.replace('.kea', '_rgb.tif')\n",
    "\n",
    "otherargs = applier.OtherInputs()\n",
    "# Using a loop to read and store each 0 column into otherargs\n",
    "for i in range(1, N_COMPONENTS+1):\n",
    "    setattr(otherargs, f'b{i}', \n",
    "            np.round(rat.readColumn(infiles.image, f'pca_Band_{i}_mean')).astype(np.uint16))\n",
    "\n",
    "otherargs.noData = 0\n",
    "controls = applier.ApplierControls()\n",
    "controls.windowxsize = controls.windowysize = 512  # Set both attributes on the same line\n",
    "controls.setReferenceImage(RASTER_SEG)\n",
    "controls.setStatsIgnore(0)\n",
    "controls.setOutputDriverName(\"GTIFF\")\n",
    "controls.setCreationOptions([\n",
    "    \"COMPRESS=DEFLATE\",\n",
    "    \"ZLEVEL=9\",\n",
    "    \"BIGTIFF=YES\",\n",
    "    \"TILED=YES\",\n",
    "    \"INTERLEAVE=BAND\",\n",
    "    \"NUM_THREADS=ALL_CPUS\",\n",
    "    \"BLOCKXSIZE=512\",\n",
    "    \"BLOCKYSIZE=512\"\n",
    "])\n",
    "\n",
    "def exportColor(info, inputs, outputs, otherargs):   \n",
    "    data = inputs.image.flatten()\n",
    "    # Access b1 through b3 from otherargs using a loop instead of individual lines\n",
    "    rgb = np.vstack([getattr(otherargs, f'b{i}')[data] for i in range(1, N_COMPONENTS+1)])\n",
    "    outputs.id = data.reshape((1,) + inputs.image.shape[1:]).astype(np.uint32)\n",
    "    outputs.rgb = rgb.reshape((rgb.shape[0],) + inputs.image.shape[1:]).astype(np.uint16)\n",
    "\n",
    "applier.apply(exportColor, infiles, outfiles, otherargs, controls=controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the RGB segmentation to inspect the segments\n",
    "plot_rgb_to_html(RASTER_SEG.replace('.kea', '_rgb.tif'), \"segmentationRGB.html\", zoom=8, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster the segments based on segment mean and std reflectance\n",
    "\n",
    "Note: This next step exracts the 'Spectral Species', however for this analysis,\n",
    "the segments from the previous step will be used instead.\n",
    "\n",
    "To determine a 'good' number of clusters, the silhouette score for 10-256 clusters will be computed. In practice, something like aglomerative clustering may produce better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use K-means to cluster the segments. This would be a categorical predictor for a SDM\n",
    "\n",
    "wantedAttributes = ['pca_Band_1_mean', 'pca_Band_1_std', 'pca_Band_2_mean', 'pca_Band_2_std', 'pca_Band_3_mean', 'pca_Band_3_std', 'pca_Band_4_mean', 'pca_Band_4_std']\n",
    "\n",
    "# Read the data into an array\n",
    "clusterData  = np.transpose([rat.readColumn(RASTER_SEG,name) for name in wantedAttributes])\n",
    "\n",
    "# Drop the first row, as it contains the nodata\n",
    "clusterData = clusterData[1:]\n",
    "\n",
    "# Scale the data\n",
    "clusterData = RobustScaler().fit_transform(clusterData)\n",
    "\n",
    "# Sample the data to reduce the number of points\n",
    "MAX_TESTING_POINTS = 2000000 # In practice, a much larger sample size would be used\n",
    "sampleSize = min(MAX_TESTING_POINTS,clusterData.shape[0])\n",
    "sampleIdx = np.random.choice(clusterData.shape[0],sampleSize,replace=False)\n",
    "\n",
    "maxClusters = 30\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def minibatch_kmeans_elbow(X, max_clusters=10, batch_size=100):\n",
    "    # Compute the total inertia (inertia with 1 cluster) for explained variance calculation\n",
    "    total_inertia = MiniBatchKMeans(n_clusters=1, batch_size=batch_size, random_state=42).fit(X).inertia_\n",
    "\n",
    "    # List to store the explained variance percentages\n",
    "    explained_variance_percent = []\n",
    "\n",
    "    # Loop over the number of clusters\n",
    "    for k in tqdm(range(1, max_clusters + 1), desc='Finding optimal number of clusters'):\n",
    "        # Initialize MiniBatchKMeans\n",
    "        kmeans = MiniBatchKMeans(n_clusters=k, batch_size=batch_size, random_state=42)\n",
    "        \n",
    "        # Fit the model\n",
    "        kmeans.fit(X)\n",
    "        \n",
    "        # Calculate explained variance percentage\n",
    "        variance_explained = 1 - (kmeans.inertia_ / total_inertia)\n",
    "        explained_variance_percent.append(variance_explained * 100)\n",
    "\n",
    "    # Plot the elbow curve for explained variance\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(range(1, max_clusters + 1), explained_variance_percent, 'bo-')\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel('Explained Variance (%)')\n",
    "    plt.title('Elbow Method using Explained Variance')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return explained_variance_percent\n",
    "\n",
    "# Example usage:\n",
    "# X is your dataset as a NumPy array or Pandas DataFrame\n",
    "variance = minibatch_kmeans_elbow(clusterData[sampleIdx], max_clusters=maxClusters)\n",
    "\n",
    "\n",
    "\n",
    "# # Find the optimal number of clusters\n",
    "# inertia = []\n",
    "# silhouette = []\n",
    "# nclasses = range(10,256)\n",
    "# for c in tqdm(nclasses, desc='Finding optimal number of clusters'):\n",
    "#     kmeans = MiniBatchKMeans(n_clusters=c).fit(clusterData[sampleIdx])\n",
    "#     inertia.append(kmeans.inertia_)\n",
    "#     silhouette.append(silhouette_score(clusterData[sampleIdx], kmeans.labels_))\n",
    "\n",
    "# # Find top 5 number of classes and silhouette score\n",
    "# top5 = np.argsort(silhouette)[::-1][:5]\n",
    "# print(f\"Top 5 number of classes: {np.array(nclasses)[top5]}\")\n",
    "\n",
    "# # Plot the silhouette score\n",
    "# plt.figure(figsize=(12,4))\n",
    "# plt.plot(nclasses, silhouette)\n",
    "# plt.grid()\n",
    "# plt.title('Clustering silhouette score')\n",
    "# plt.xlabel('Number of clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the data using K-means and write to a csv with other Segment columns\n",
    "OPTIMAL_NUM_CLUSTERS = 30\n",
    "# Cluster the full dataset\n",
    "kmeans = MiniBatchKMeans(n_clusters = OPTIMAL_NUM_CLUSTERS).fit(clusterData)\n",
    "# Get the cluster labels and add one to avoid zero values\n",
    "clusterLabels = kmeans.labels_ + 1\n",
    "# Add back in the first row of nodata\n",
    "clusterLabels = np.insert(clusterLabels,0,0)\n",
    "\n",
    "# Write cluster labels to kea\n",
    "rat.writeColumn(RASTER_SEG, f'clust{OPTIMAL_NUM_CLUSTERS}', np.insert(kmeans.labels_ + 1, 0, 0))\n",
    "\n",
    "# Write to .csv\n",
    "# Get the column names\n",
    "colNames = rat.getColumnNames(RASTER_SEG)\n",
    "# Read the data into an array\n",
    "allData = np.transpose([rat.readColumn(RASTER_SEG, name) for name in colNames])\n",
    "# Add an index column\n",
    "allData = np.vstack((np.arange(len(allData)),allData.T)).T\n",
    "# Add the kmeans labels\n",
    "allData = np.vstack((allData.T,clusterLabels)).T\n",
    "# Make a header\n",
    "colNames.insert(0,'segid')\n",
    "colNames.append('kmeans_label')\n",
    "header = ','.join(colNames)\n",
    "# Write the file\n",
    "np.savetxt(RASTER_SEG.replace(\".kea\", \".csv\"),allData,fmt='%8.3f',delimiter=',',header=header,comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Depending on data sharing, a new Geopackage could be created with the segmentation polygons\n",
    "\n",
    "# Append the segmentation to an existing Geopackage\n",
    "outDs = gdal.Open(RASTER_SEG.replace(\".kea\", \"_id.tif\"))\n",
    "band = outDs.GetRasterBand(1) # Band 1 is the segment ids\n",
    "# Open the existing vector datasource\n",
    "vecDs = ogr.Open(AOI, 1)  # 1 means it's opened in update mode\n",
    "# Create a new layer in the existing geopackage\n",
    "vecLayer = vecDs.CreateLayer('BrigalowSegID', srs=osr.SpatialReference(wkt=outDs.GetProjection()))\n",
    "vecLayer.CreateField(ogr.FieldDefn('segid', ogr.OFTInteger))\n",
    "# Polygonize\n",
    "vecDs.StartTransaction()\n",
    "gdal.Polygonize(band, None, vecLayer, 0, [], callback=None)\n",
    "vecDs.CommitTransaction()\n",
    "\n",
    "del outDs\n",
    "del vecDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the segmentation to a new Geopackage\n",
    "\n",
    "segDS = RASTER_SEG.replace(\".kea\", \"_segid.gpkg\")\n",
    "\n",
    "outDs = gdal.Open(RASTER_SEG.replace(\".kea\", \"_id.tif\"))\n",
    "band = outDs.GetRasterBand(1)  # Band 1 is the segment ids\n",
    "\n",
    "# Create a new vector datasource\n",
    "driver = ogr.GetDriverByName(\"GPKG\")\n",
    "if os.path.exists(segDS):\n",
    "    driver.DeleteDataSource(segDS)\n",
    "vecDs = driver.CreateDataSource(segDS)\n",
    "\n",
    "# Create a new layer in the new geopackage\n",
    "vecLayer = vecDs.CreateLayer('BrigalowSegID', srs=osr.SpatialReference(wkt=outDs.GetProjection()))\n",
    "vecLayer.CreateField(ogr.FieldDefn('segid', ogr.OFTInteger))\n",
    "\n",
    "# Polygonize\n",
    "vecDs.StartTransaction()\n",
    "gdal.Polygonize(band, None, vecLayer, 0, [], callback=None)\n",
    "vecDs.CommitTransaction()\n",
    "\n",
    "del outDs\n",
    "del vecDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "segDS = AOI.replace(\".gpkg\", \"_segs.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join segments to attributes .csv\n",
    "Segs = gpd.read_file(segDS, layer='BrigalowSegID')\n",
    "attributes = gpd.read_file(RASTER_SEG.replace(\".kea\", \".csv\"))\n",
    "\n",
    "\n",
    "# Join on segid\n",
    "attributes['segid'] = attributes['segid'].astype(float).astype(int)\n",
    "attributes['kmeans_label'] = attributes['kmeans_label'].astype(float).astype(int)\n",
    "Segs = Segs.merge(attributes, on='segid')\n",
    "Segs.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get an example species and test Species Distribution Modelling on the Spectral Species\n",
    "\n",
    "For the proof of concept, I've picked a species of Casuarina, on the assumption that they have distinct reflectivity based on their physiology. This may or may not hold true at 250m pixels, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get species data from ALA API\n",
    "# Use the bbox from the AOI to get the occurrences\n",
    "\n",
    "import galah.atlas_citation\n",
    "\n",
    "\n",
    "bbox = box(*brigalow.total_bounds)\n",
    "\n",
    "# Coords need to be in 4283/4326\n",
    "\n",
    "brigalow_occ = galah.atlas_occurrences(bbox = bbox, \n",
    "                                    taxa=['plantae'],\n",
    "                                    filters=[\"year>=2014\"],\n",
    "                                    fields=[\"decimalLatitude\",\n",
    "                                            \"decimalLongitude\",\n",
    "                                            \"eventDate\", \n",
    "                                            \"scientificName\",\n",
    "                                            \"vernacularName\",\n",
    "                                            \"coordinateUncertaintyInMeters\"],\n",
    "                                )\n",
    "# Make a geodataframe from the lat and longs\n",
    "brigalow_occ = gpd.GeoDataFrame(\n",
    "    brigalow_occ, \n",
    "    geometry=gpd.points_from_xy(brigalow_occ.decimalLongitude, brigalow_occ.decimalLatitude),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "brigalow_occ.to_crs(3577, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brigalow_occ.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Segs = gpd.read_file(\"/home/tim/rubella/scripts/SpectralBotany/data/SpectralBotanyTest_Casuarina_cristata.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Segs = Segs.drop(columns=['presence', 'presence_probabilities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dataframe with presence data and join to Segment data\n",
    "# Filter the DataFrame for \"Casuarina cristata\"\n",
    "filtered_gdf = brigalow_occ[brigalow_occ['scientificName'].str.contains(\"Dichanthium sericeum\")].copy()\n",
    "\n",
    "# Replace spaces with underscores in species names\n",
    "filtered_gdf.loc[:, 'species_name'] = filtered_gdf['scientificName'].str.replace(\" \", \"_\")\n",
    "\n",
    "# Set presence to 1\n",
    "filtered_gdf.loc[:, 'presence'] = 1\n",
    "\n",
    "# Drop unneeded columns\n",
    "filtered_gdf.drop(columns=['decimalLatitude', 'decimalLongitude', 'eventDate', 'scientificName', 'vernacularName'], inplace=True)\n",
    "\n",
    "# Intersect with segments\n",
    "Segs_intersected = gpd.overlay(Segs, filtered_gdf, how='intersection', keep_geom_type=False)\n",
    "\n",
    "Segs.loc[Segs['segid'].isin(Segs_intersected['segid']), 'presence'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pseudoabsences\n",
    "\n",
    "Absence_ratio = 5 # Note: I can't remember the research for absence/presence ratios in SDMs, but this will do for testing\n",
    "\n",
    "Segs_presence = Segs.copy()\n",
    "\n",
    "Segs_presence = Segs_presence[Segs_presence['segid'] != 0] # Remove the no data segment\n",
    "\n",
    "# Get number of presences\n",
    "num_presence = len(Segs_presence[Segs_presence['presence'] == 1])\n",
    "\n",
    "# Randomly select rows by segid\n",
    "random_rows = Segs_presence.sample(n=num_presence*Absence_ratio, random_state=42)\n",
    "\n",
    "# Set these segments to absent '0'\n",
    "Segs_presence.loc[Segs_presence['segid'].isin(random_rows['segid']), 'presence'] = 0\n",
    "\n",
    "# Print number of presences and absences in the dataset\n",
    "num_presence = len(Segs_presence[Segs_presence['presence'] == 1])\n",
    "num_absence = len(Segs_presence[Segs_presence['presence'] == 0])\n",
    "print(\"Number of presences:\", num_presence)\n",
    "print(\"Number of absences:\", num_absence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a csv\n",
    "speciesDS = AOI.replace(\".gpkg\", \"_dichanthium.csv\") # Write to a new geopackage to enable upload to git\n",
    "Segs_presence.to_csv(speciesDS, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "\n",
    "# Make points from centroid of polygon\n",
    "points = Segs_presence.copy()\n",
    "points['centroid'] = points.centroid\n",
    "points.set_geometry('centroid', inplace=True)\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the presence column with a red/blue colormap\n",
    "points.plot(column='presence', cmap='bwr', ax=ax, legend=True)\n",
    "\n",
    "# Set the title and axis labels\n",
    "ax.set_title('Presence/Absence')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the points to a geopackage\n",
    "points['geometry'] = points['centroid']\n",
    "points.drop(columns=['centroid'], inplace=True)\n",
    "points.to_file(\"./data/Dichanthium.geojson\", driver='GeoJSON')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Random Forest to estimate Casuarina species distribution\n",
    "\n",
    "This is a basic test on a Casuarina species as a proof of concept for generating a SDM from spectral species data. Segment mean reflectance and variability (Standard Deviation)\n",
    "will be used as predictors in a Random Forest model to predict the probability of occurance of Casuarina castata in the Brigalow Belt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make test/train datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = 42 # Set a random seed for reproducibility\n",
    "\n",
    "presAbs = Segs_presence.dropna(subset=['presence']) # split data to the training set and the full ds\n",
    "\n",
    "predictors = ['pca_Band_1_mean', 'pca_Band_1_std', 'pca_Band_2_mean',\n",
    "              'pca_Band_2_std', 'pca_Band_3_mean', 'pca_Band_3_std'] \n",
    "\n",
    "# Get the x and y data with only spectral predictors\n",
    "x = presAbs[predictors]\n",
    "y = presAbs['presence']\n",
    "\n",
    "# Split training set into test and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Random Forest model\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Creating a pipeline that includes scaling and the random forest classifier\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('classifier', RandomForestClassifier(random_state=random_state))\n",
    "])\n",
    "\n",
    "# Training the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the validation set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Extracting feature importances\n",
    "feature_importances = pipeline.named_steps['classifier'].feature_importances_\n",
    "most_important_feature = predictors[feature_importances.argmax()]\n",
    "importance_value = feature_importances.max()\n",
    "\n",
    "# Print pipeline, accuracy, report, most_important_feature, and importance_value\n",
    "print(\"Pipeline:\", pipeline)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Plot predictor importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(predictors, feature_importances)\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Predictor')\n",
    "plt.title('Predictor Importances')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the full dataset\n",
    "# Getting probabilities for all classes\n",
    "probabilities = pipeline.predict_proba(Segs_presence[predictors])\n",
    "\n",
    "# Get model classes to check order\n",
    "classes = pipeline.classes_\n",
    "print(\"Class order in model:\", classes)  # This line will print the order of classes\n",
    "\n",
    "# Class '1' is 'presence', find its index\n",
    "presence_index = list(classes).index(1)  # Change 1 to the appropriate class label if different\n",
    "\n",
    "# extract probabilities for the 'presence' class\n",
    "Segs_presence['presence_probabilities'] = probabilities[:, presence_index]\n",
    "\n",
    "# Scale presence probabilities to 0 - 100 and convert to integer\n",
    "Segs_presence['presence_probabilities'] = (Segs_presence['presence_probabilities'] * 100).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to a gpkg\n",
    "Segs_presence.to_file(speciesDS.replace(\".csv\", \".gpkg\"), layer='Dichanthium', driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal, ogr\n",
    "\n",
    "# Rasterise the presence probabilities for a better visual\n",
    "\n",
    "# Create a new raster file\n",
    "outRaster = RASTER_SEG.replace(\".kea\", \"Dichanthium_prob.tif\")\n",
    "\n",
    "# Open the geopackage\n",
    "gpkg_ds = ogr.Open(speciesDS.replace(\".csv\", \".gpkg\"))\n",
    "\n",
    "# Get the layer from the geopackage\n",
    "layer = gpkg_ds.GetLayerByName(\"Dichanthium\")\n",
    "\n",
    "# Get the field index of 'presence_probabilities'\n",
    "field_index = layer.GetLayerDefn().GetFieldIndex(\"presence_probabilities\")\n",
    "\n",
    "# Define the pixel size (250m)\n",
    "pixel_size = 250\n",
    "\n",
    "# Get the extent of the layer\n",
    "x_min, x_max, y_min, y_max = layer.GetExtent()\n",
    "\n",
    "# Calculate the number of pixels\n",
    "x_res = int((x_max - x_min) / pixel_size)\n",
    "y_res = int((y_max - y_min) / pixel_size)\n",
    "\n",
    "# Create the output raster\n",
    "target_ds = gdal.GetDriverByName('GTiff').Create(outRaster, x_res, y_res, 1, gdal.GDT_Byte)\n",
    "target_ds.SetGeoTransform((x_min, pixel_size, 0, y_max, 0, -pixel_size))\n",
    "\n",
    "# Set the projection from the layer\n",
    "target_ds.SetProjection(layer.GetSpatialRef().ExportToWkt())\n",
    "\n",
    "# Set NoData value to 255\n",
    "band = target_ds.GetRasterBand(1)\n",
    "band.SetNoDataValue(255)\n",
    "\n",
    "# Rasterize the layer based on the 'presence_probabilities' field\n",
    "gdal.RasterizeLayer(target_ds, [1], layer, options=[\"ATTRIBUTE=presence_probabilities\"])\n",
    "\n",
    "# Close the datasets\n",
    "target_ds = None\n",
    "gpkg_ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get outraster no data\n",
    "ds = gdal.Open(outRaster)\n",
    "noData = ds.GetRasterBand(1).GetNoDataValue()\n",
    "\n",
    "plot_thematic_to_html(outRaster, \"Dichathium_prob.html\", zoom=8, nodata=noData, legend_range=[0, 100], show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "\n",
    "filelist = glob.glob(\"/home/tim/dentata/Sentinel2_seasonal/*20[19-24]*.vrt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
